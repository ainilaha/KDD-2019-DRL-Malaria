{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [KDD Cup|Humanities Track Tutorial Q-Learning](https://compete.hexagon-ml.com/tutorial/kdd-cuphumanities-track-tutorial/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KDD Cup|Humanities Track Tutorial Q-Learning\n",
    "This Tutorial builds on the previous tutorial to demonstrate a baseline implementation of a standard Reinforcement Learning (RL) Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### State\n",
    "\n",
    "$S \\in \\{1,2,3,4,5\\}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Action\n",
    "$A_S = [a_{ITN},a_{IRS}]$\n",
    "\n",
    "where  $a_{ITN} \\in [0,1]$ and $a_{IRS} \\in [0,1]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reward\n",
    "$R_{\\pi} \\in (- \\infty,\\infty)$\n",
    "\n",
    "![](image/rewards2.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "import random\n",
    "# !pip3 install git+https://github.com/slremy/netsapi --user --upgrade\n",
    "from netsapi.challenge import * "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning a Value Function Based on Ïµ-greedy action selection\n",
    "\n",
    "This common resource was used as a reference for the implementation presented here: https://kofzor.github.io/Learning_Value_Functions/. Please refer to the blog and this Tutorial in tandem. The code below uses the first example from the blog with the Challenge Environment (as opposed to Gym)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "actions.shape= 121\n",
      "state= 1\n",
      "action= 0\n",
      "env_action <class 'list'>\n",
      "10500  Evaluations Remaining\n",
      "reward= 2.9052422663040014\n",
      "done= False\n",
      "state= 2\n",
      "action= 0\n",
      "env_action <class 'list'>\n",
      "10499  Evaluations Remaining\n",
      "reward= -0.1596777840804906\n",
      "done= False\n",
      "state= 3\n",
      "action= 0\n",
      "env_action <class 'list'>\n",
      "10498  Evaluations Remaining\n",
      "reward= 0.07281273286044732\n",
      "done= False\n",
      "state= 4\n",
      "action= 0\n",
      "env_action <class 'list'>\n",
      "10497  Evaluations Remaining\n",
      "reward= -0.1395876236706961\n",
      "done= False\n",
      "state= 5\n",
      "action= 0\n",
      "env_action <class 'list'>\n",
      "10496  Evaluations Remaining\n",
      "reward= -0.052668676708988116\n",
      "done= True\n",
      "10495  Evaluations Remaining\n",
      "{1: [0.0, 0.0], 2: [0.0, 0.0], 3: [0.1, 0.0], 4: [0.0, 0.0], 5: [0.1, 0.0]} 1.9885433006855462\n"
     ]
    }
   ],
   "source": [
    "env = ChallengeSeqDecEnvironment(experimentCount = 10500)\n",
    "\n",
    "Q = defaultdict(lambda : 0.) # Q-function\n",
    "n = defaultdict(lambda : 1.) # number of visits\n",
    "\n",
    "def actionSpace(resolution):\n",
    "    x,y = np.meshgrid(np.arange(0,1+resolution,resolution), np.arange(0,1+resolution,resolution))\n",
    "    xy = np.concatenate((x.reshape(-1,1), y.reshape(-1,1)), axis=1)\n",
    "    return xy.round(2).tolist()\n",
    "\n",
    "#HyperParameters\n",
    "epsilon = 0.1\n",
    "gamma = 0.9\n",
    "action_resolution = 0.1\n",
    "episode_number = 1 #for submission this is fixed as 20\n",
    "\n",
    "\n",
    "#Set-up\n",
    "actions = actionSpace(action_resolution)\n",
    "print(\"actions.shape=\",len(actions))\n",
    "actionspace = range(len(actions)-1)\n",
    "greedy_action = lambda s : max(actionspace, key=lambda a : Q[(s,a)])\n",
    "max_q = lambda sp : max([Q[(sp,a)] for a in actionspace])\n",
    "\n",
    "#Training of Q Table\n",
    "for _ in range(episode_number):\n",
    "    env.reset()\n",
    "    nextstate = env.state\n",
    "    while True:\n",
    "        state = nextstate\n",
    "        print(\"state=\",state)\n",
    "        # Epsilon-Greedy\n",
    "        if epsilon > random.random() :\n",
    "            action = random.choice(actionspace)\n",
    "            print('random_action',action)\n",
    "        else :\n",
    "            action = greedy_action(state)\n",
    "        print(\"action=\",action)    \n",
    "        env_action = actions[action] #convert to ITN/IRS\n",
    "        print('env_action', type(env_action))\n",
    "        nextstate, reward, done, _ = env.evaluateAction(env_action)\n",
    "        print('reward=',reward)\n",
    "        print('done=',done)\n",
    "        \n",
    "\n",
    "        # Q-learning\n",
    "        if done :\n",
    "            Q[(state,action)] = Q[(state,action)] + \\\n",
    "                                1./n[(state,action)] * ( reward - Q[(state,action)] )\n",
    "            break\n",
    "        else :\n",
    "            Q[(state,action)] = Q[(state,action)] + \\\n",
    "                                1./n[(state,action)] * ( reward + \\\n",
    "                                                        gamma * max_q(nextstate) - Q[(state,action)] )\n",
    "\n",
    "#Greedy Policy Learnt from Q Table\n",
    "best_policy = {state: list(actions[greedy_action(state-1)]) for state in range(1,6)}\n",
    "best_reward = env.evaluatePolicy(best_policy)\n",
    "print(best_policy, best_reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a Valid Submission from Agent Code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Q_Agent():\n",
    "    \n",
    "    def __init__(self, environment):\n",
    "        \n",
    "        #Hyperparameters\n",
    "        self.env = environment\n",
    "        self.epsilon = 0.1\n",
    "        self.gamma = 0.9\n",
    "        self.action_resolution = 0.2\n",
    "        self.Q = defaultdict(lambda : 0.) # Q-function\n",
    "        self.n = defaultdict(lambda : 1.) # number of visits\n",
    "        self.actions = actionSpace(self.action_resolution)\n",
    "        self.actionspace = range(len(self.actions)-1)\n",
    "        \n",
    "    \n",
    "    def actionSpace(self):\n",
    "        x,y = np.meshgrid(np.arange(0,1+self.action_resolution,self.action_resolution),\n",
    "                          np.arange(0,1+self.action_resolution,self.action_resolution))\n",
    "        xy = np.concatenate((x.reshape(-1,1), y.reshape(-1,1)), axis=1)\n",
    "        return xy.round(2).tolist()\n",
    "\n",
    "    def train(self):\n",
    "        \n",
    "        Q = self.Q\n",
    "        n = self.n\n",
    "        actions = self.actions\n",
    "        actionspace = self.actionspace\n",
    "\n",
    "        greedy_action = lambda s : max(actionspace, key=lambda a : Q[(s,a)])\n",
    "        max_q = lambda sp : max([Q[(sp,a)] for a in actionspace])\n",
    "\n",
    "        \n",
    "        for _ in range(200): #Do not change\n",
    "            \n",
    "            self.env.reset()\n",
    "            nextstate = self.env.state\n",
    "            \n",
    "            while True:\n",
    "                state = nextstate\n",
    "\n",
    "                # Epsilon-Greedy Action Selection\n",
    "                if epsilon > random.random() :\n",
    "                    action = random.choice(actionspace)\n",
    "                else :\n",
    "                    action = greedy_action(state)\n",
    "\n",
    "                env_action = actions[action]#convert to ITN/IRS\n",
    "                print('env_action', env_action)\n",
    "                nextstate, reward, done, _ = self.env.evaluateAction(env_action)\n",
    "                print(\"nextstate=\",nextstate)\n",
    "                print(\"reward=\",reward)\n",
    "\n",
    "                # Q-learning\n",
    "                if done :\n",
    "                    Q[(state,action)] = Q[(state,action)] + 1./n[(state,action)] * ( reward - Q[(state,action)] )\n",
    "                    break\n",
    "                else :\n",
    "                    Q[(state,action)] = Q[(state,action)] + 1./n[(state,action)] * ( reward + gamma * max_q(nextstate) - Q[(state,action)] )\n",
    "\n",
    "        return Q\n",
    "\n",
    "\n",
    "    def generate(self):\n",
    "        best_policy = None\n",
    "        best_reward = -float('Inf')\n",
    "        \n",
    "        Q_trained = self.train()\n",
    "        greedy_eval = lambda s : max(actionspace, key=lambda a : Q_trained[(s,a)])\n",
    "        \n",
    "        best_policy = {state: list(actions[greedy_eval(state-1)]) for state in range(1,6)}\n",
    "        best_reward = self.env.evaluatePolicy(best_policy)\n",
    "        \n",
    "        print(best_policy, best_reward)\n",
    "        \n",
    "        return best_policy, best_reward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the EvaluateChallengeSubmission Method with your Agent Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env_action [0.0, 0.0]\n",
      "105  Evaluations Remaining\n",
      "nextstate= 2\n",
      "reward= 2.469036599869387\n",
      "env_action [0.0, 0.0]\n",
      "104  Evaluations Remaining\n",
      "nextstate= 3\n",
      "reward= 0.18492713129336869\n",
      "env_action [0.0, 0.0]\n",
      "103  Evaluations Remaining\n",
      "nextstate= 4\n",
      "reward= 0.11851761849252807\n",
      "env_action [0.0, 0.0]\n",
      "102  Evaluations Remaining\n",
      "nextstate= 5\n",
      "reward= 0.049422308420226546\n",
      "env_action [0.0, 0.0]\n",
      "101  Evaluations Remaining\n",
      "nextstate= 6\n",
      "reward= 0.1319613649711946\n",
      "env_action [0.0, 0.0]\n",
      "100  Evaluations Remaining\n",
      "nextstate= 2\n",
      "reward= 2.5733415915866735\n",
      "env_action [0.0, 0.0]\n",
      "99  Evaluations Remaining\n",
      "nextstate= 3\n",
      "reward= -0.2234729786123344\n",
      "env_action [0.0, 0.0]\n",
      "98  Evaluations Remaining\n",
      "nextstate= 4\n",
      "reward= 0.06848864193618898\n",
      "env_action [0.0, 0.0]\n",
      "97  Evaluations Remaining\n",
      "nextstate= 5\n",
      "reward= 0.042680290439070845\n",
      "env_action [0.0, 0.0]\n",
      "96  Evaluations Remaining\n",
      "nextstate= 6\n",
      "reward= 0.14730562992361618\n",
      "env_action [0.0, 0.0]\n",
      "95  Evaluations Remaining\n",
      "nextstate= 2\n",
      "reward= 2.4998379837878053\n",
      "env_action [0.01, 0.0]\n",
      "94  Evaluations Remaining\n",
      "nextstate= 3\n",
      "reward= 0.060195095578418734\n",
      "env_action [0.0, 0.0]\n",
      "93  Evaluations Remaining\n",
      "nextstate= 4\n",
      "reward= -0.011225526832717936\n",
      "env_action [0.0, 0.0]\n",
      "92  Evaluations Remaining\n",
      "nextstate= 5\n",
      "reward= -0.14161236311202163\n",
      "env_action [0.0, 0.0]\n",
      "91  Evaluations Remaining\n",
      "nextstate= 6\n",
      "reward= -0.06243643730089854\n",
      "env_action [0.0, 0.0]\n",
      "90  Evaluations Remaining\n",
      "nextstate= 2\n",
      "reward= 2.4595805957514605\n",
      "env_action [0.25, 0.97]\n",
      "89  Evaluations Remaining\n",
      "nextstate= 3\n",
      "reward= 56.791644803526594\n",
      "env_action [0.0, 0.0]\n",
      "88  Evaluations Remaining\n",
      "nextstate= 4\n",
      "reward= -0.026875878312691004\n",
      "env_action [0.01, 0.0]\n",
      "87  Evaluations Remaining\n",
      "nextstate= 5\n",
      "reward= 0.08728411169103767\n",
      "env_action [0.01, 0.0]\n",
      "86  Evaluations Remaining\n",
      "nextstate= 6\n",
      "reward= 0.03501925852490961\n",
      "env_action [0.0, 0.0]\n",
      "85  Evaluations Remaining\n",
      "nextstate= 2\n",
      "reward= 2.9868873243016374\n",
      "env_action [0.25, 0.97]\n",
      "84  Evaluations Remaining\n",
      "nextstate= 3\n",
      "reward= 56.29259865406746\n",
      "env_action [0.01, 0.0]\n",
      "83  Evaluations Remaining\n",
      "nextstate= 4\n",
      "reward= -0.06832208686507046\n",
      "env_action [0.01, 0.0]\n",
      "82  Evaluations Remaining\n",
      "nextstate= 5\n",
      "reward= 0.07259899241604906\n",
      "env_action [0.01, 0.0]\n",
      "81  Evaluations Remaining\n",
      "nextstate= 6\n",
      "reward= -0.08862703119913817\n",
      "env_action [0.0, 0.0]\n",
      "80  Evaluations Remaining\n",
      "nextstate= 2\n",
      "reward= 2.7788286362681416\n",
      "env_action [0.25, 0.97]\n",
      "79  Evaluations Remaining\n",
      "nextstate= 3\n",
      "reward= 56.37028895524493\n",
      "env_action [0.01, 0.0]\n",
      "78  Evaluations Remaining\n",
      "nextstate= 4\n",
      "reward= -0.2288426120352236\n",
      "env_action [0.01, 0.0]\n",
      "77  Evaluations Remaining\n",
      "nextstate= 5\n",
      "reward= 0.09970863028816934\n",
      "env_action [0.02, 0.0]\n",
      "76  Evaluations Remaining\n",
      "nextstate= 6\n",
      "reward= -0.19049998643984134\n",
      "env_action [0.0, 0.0]\n",
      "75  Evaluations Remaining\n",
      "nextstate= 2\n",
      "reward= 2.821588812388495\n",
      "env_action [0.25, 0.97]\n",
      "74  Evaluations Remaining\n",
      "nextstate= 3\n",
      "reward= 57.961146991084284\n",
      "env_action [0.02, 0.0]\n",
      "73  Evaluations Remaining\n",
      "nextstate= 4\n",
      "reward= 0.11037134556084993\n",
      "env_action [0.01, 0.0]\n",
      "72  Evaluations Remaining\n",
      "nextstate= 5\n",
      "reward= -0.0477707458729415\n",
      "env_action [0.03, 0.0]\n",
      "71  Evaluations Remaining\n",
      "nextstate= 6\n",
      "reward= 0.2260969929776966\n",
      "env_action [0.0, 0.0]\n",
      "70  Evaluations Remaining\n",
      "nextstate= 2\n",
      "reward= 2.9887138929366515\n",
      "env_action [0.25, 0.97]\n",
      "69  Evaluations Remaining\n",
      "nextstate= 3\n",
      "reward= 54.852197330142005\n",
      "env_action [0.02, 0.0]\n",
      "68  Evaluations Remaining\n",
      "nextstate= 4\n",
      "reward= -0.1411614698519843\n",
      "env_action [0.02, 0.0]\n",
      "67  Evaluations Remaining\n",
      "nextstate= 5\n",
      "reward= 0.2347409710655204\n",
      "env_action [0.03, 0.0]\n",
      "66  Evaluations Remaining\n",
      "nextstate= 6\n",
      "reward= -0.1526899662326202\n",
      "env_action [0.0, 0.0]\n",
      "65  Evaluations Remaining\n",
      "nextstate= 2\n",
      "reward= 2.612604937443613\n",
      "env_action [0.25, 0.97]\n",
      "64  Evaluations Remaining\n",
      "nextstate= 3\n",
      "reward= 53.38478170768018\n",
      "env_action [0.03, 0.0]\n",
      "63  Evaluations Remaining\n",
      "nextstate= 4\n",
      "reward= 0.14253123782246702\n",
      "env_action [0.02, 0.0]\n",
      "62  Evaluations Remaining\n",
      "nextstate= 5\n",
      "reward= 0.07685429529070165\n",
      "env_action [0.04, 0.0]\n",
      "61  Evaluations Remaining\n",
      "nextstate= 6\n",
      "reward= -0.2937729345228055\n",
      "env_action [0.0, 0.0]\n",
      "60  Evaluations Remaining\n",
      "nextstate= 2\n",
      "reward= 2.5375312365744005\n",
      "env_action [0.25, 0.97]\n",
      "59  Evaluations Remaining\n",
      "nextstate= 3\n",
      "reward= 57.33066991740129\n",
      "env_action [0.03, 0.0]\n",
      "58  Evaluations Remaining\n",
      "nextstate= 4\n",
      "reward= -0.2784645180652605\n",
      "env_action [0.02, 0.0]\n",
      "57  Evaluations Remaining\n",
      "nextstate= 5\n",
      "reward= 0.2331344349797404\n",
      "env_action [0.05, 0.0]\n",
      "56  Evaluations Remaining\n",
      "nextstate= 6\n",
      "reward= 0.12645279158968403\n",
      "env_action [0.0, 0.0]\n",
      "55  Evaluations Remaining\n",
      "nextstate= 2\n",
      "reward= 2.578694096792058\n",
      "env_action [0.25, 0.97]\n",
      "54  Evaluations Remaining\n",
      "nextstate= 3\n",
      "reward= 56.29238346918322\n",
      "env_action [0.04, 0.0]\n",
      "53  Evaluations Remaining\n",
      "nextstate= 4\n",
      "reward= 0.16343409045757484\n",
      "env_action [0.02, 0.0]\n",
      "52  Evaluations Remaining\n",
      "nextstate= 5\n",
      "reward= 0.014988188588243023\n",
      "env_action [0.05, 0.0]\n",
      "51  Evaluations Remaining\n",
      "nextstate= 6\n",
      "reward= 0.1341751586351827\n",
      "env_action [0.0, 0.0]\n",
      "50  Evaluations Remaining\n",
      "nextstate= 2\n",
      "reward= 2.4975815530599386\n",
      "env_action [0.25, 0.97]\n",
      "49  Evaluations Remaining\n",
      "nextstate= 3\n",
      "reward= 54.44391690068455\n",
      "env_action [0.04, 0.0]\n",
      "48  Evaluations Remaining\n",
      "nextstate= 4\n",
      "reward= -0.08249976459239816\n",
      "env_action [0.02, 0.0]\n",
      "47  Evaluations Remaining\n",
      "nextstate= 5\n",
      "reward= 0.04845184714204942\n",
      "env_action [0.05, 0.0]\n",
      "46  Evaluations Remaining\n",
      "nextstate= 6\n",
      "reward= -0.045053301430719284\n",
      "env_action [0.0, 0.0]\n",
      "45  Evaluations Remaining\n",
      "nextstate= 2\n",
      "reward= 2.791687429674864\n",
      "env_action [0.25, 0.97]\n",
      "44  Evaluations Remaining\n",
      "nextstate= 3\n",
      "reward= 54.181013598275754\n",
      "env_action [0.04, 0.0]\n",
      "43  Evaluations Remaining\n",
      "nextstate= 4\n",
      "reward= 0.025576722882941993\n",
      "env_action [0.02, 0.0]\n",
      "42  Evaluations Remaining\n",
      "nextstate= 5\n",
      "reward= 0.049831093028283036\n",
      "env_action [0.06, 0.0]\n",
      "41  Evaluations Remaining\n",
      "nextstate= 6\n",
      "reward= -0.32346957742141447\n",
      "env_action [0.93, 0.18]\n",
      "40  Evaluations Remaining\n",
      "nextstate= 2\n",
      "reward= -46.86908738103967\n",
      "env_action [0.25, 0.97]\n",
      "39  Evaluations Remaining\n",
      "nextstate= 3\n",
      "reward= 107.59973786941943\n",
      "env_action [0.04, 0.0]\n",
      "38  Evaluations Remaining\n",
      "nextstate= 4\n",
      "reward= 0.20349264960136226\n",
      "env_action [0.02, 0.0]\n",
      "37  Evaluations Remaining\n",
      "nextstate= 5\n",
      "reward= 0.1776453083467695\n",
      "env_action [0.07, 0.0]\n",
      "36  Evaluations Remaining\n",
      "nextstate= 6\n",
      "reward= -0.34767128729198715\n",
      "env_action [0.0, 0.0]\n",
      "35  Evaluations Remaining\n",
      "nextstate= 2\n",
      "reward= 2.4829764038250315\n",
      "env_action [0.25, 0.97]\n",
      "34  Evaluations Remaining\n",
      "nextstate= 3\n",
      "reward= 50.836154240461376\n",
      "env_action [0.04, 0.0]\n",
      "33  Evaluations Remaining\n",
      "nextstate= 4\n",
      "reward= -0.00542144115689025\n",
      "env_action [0.02, 0.0]\n",
      "32  Evaluations Remaining\n",
      "nextstate= 5\n",
      "reward= 0.14158338609394594\n",
      "env_action [0.08, 0.0]\n",
      "31  Evaluations Remaining\n",
      "nextstate= 6\n",
      "reward= 0.034435638628719456\n",
      "env_action [0.0, 0.0]\n",
      "30  Evaluations Remaining\n",
      "nextstate= 2\n",
      "reward= 2.876514275679846\n",
      "env_action [0.91, 0.51]\n",
      "29  Evaluations Remaining\n",
      "nextstate= 3\n",
      "reward= 53.728290799451734\n",
      "env_action [0.04, 0.0]\n",
      "28  Evaluations Remaining\n",
      "nextstate= 4\n",
      "reward= -0.17706888051104697\n",
      "env_action [0.92, 0.34]\n",
      "27  Evaluations Remaining\n",
      "nextstate= 5\n",
      "reward= 31.189772541562977\n",
      "env_action [0.08, 0.0]\n",
      "26  Evaluations Remaining\n",
      "nextstate= 6\n",
      "reward= 0.13953424856084018\n",
      "env_action [0.0, 0.0]\n",
      "25  Evaluations Remaining\n",
      "nextstate= 2\n",
      "reward= 2.5687992426555466\n",
      "env_action [0.91, 0.51]\n",
      "24  Evaluations Remaining\n",
      "nextstate= 3\n",
      "reward= 57.57008432833931\n",
      "env_action [0.05, 0.0]\n",
      "23  Evaluations Remaining\n",
      "nextstate= 4\n",
      "reward= -0.18910901205970587\n",
      "env_action [0.92, 0.34]\n",
      "22  Evaluations Remaining\n",
      "nextstate= 5\n",
      "reward= 28.671307715873237\n",
      "env_action [0.08, 0.0]\n",
      "21  Evaluations Remaining\n",
      "nextstate= 6\n",
      "reward= -0.17964590350008036\n",
      "env_action [0.0, 0.0]\n",
      "20  Evaluations Remaining\n",
      "nextstate= 2\n",
      "reward= 2.590017839236977\n",
      "env_action [0.91, 0.51]\n",
      "19  Evaluations Remaining\n",
      "nextstate= 3\n",
      "reward= 54.69733039806447\n",
      "env_action [0.05, 0.0]\n",
      "18  Evaluations Remaining\n",
      "nextstate= 4\n",
      "reward= -0.09238205795943122\n",
      "env_action [0.92, 0.34]\n",
      "17  Evaluations Remaining\n",
      "nextstate= 5\n",
      "reward= 32.200756570721886\n",
      "env_action [0.09, 0.0]\n",
      "16  Evaluations Remaining\n",
      "nextstate= 6\n",
      "reward= 0.1653532720877422\n",
      "env_action [0.0, 0.0]\n",
      "15  Evaluations Remaining\n",
      "nextstate= 2\n",
      "reward= 2.486757682393478\n",
      "env_action [0.91, 0.51]\n",
      "14  Evaluations Remaining\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nextstate= 3\n",
      "reward= 52.715558539605176\n",
      "env_action [0.05, 0.0]\n",
      "13  Evaluations Remaining\n",
      "nextstate= 4\n",
      "reward= -0.06410038351457814\n",
      "env_action [0.92, 0.34]\n",
      "12  Evaluations Remaining\n",
      "nextstate= 5\n",
      "reward= 29.21438135843079\n",
      "env_action [0.09, 0.0]\n",
      "11  Evaluations Remaining\n",
      "nextstate= 6\n",
      "reward= 0.05961615108611307\n",
      "env_action [0.76, 0.73]\n",
      "10  Evaluations Remaining\n",
      "nextstate= 2\n",
      "reward= 5.837346858028075\n",
      "env_action [0.91, 0.51]\n",
      "9  Evaluations Remaining\n",
      "nextstate= 3\n",
      "reward= -46.878764133268945\n",
      "env_action [0.05, 0.0]\n",
      "8  Evaluations Remaining\n",
      "nextstate= 4\n",
      "reward= 0.05593157675307037\n",
      "env_action [0.92, 0.34]\n",
      "7  Evaluations Remaining\n",
      "nextstate= 5\n",
      "reward= 30.60929909735881\n",
      "env_action [0.09, 0.0]\n",
      "6  Evaluations Remaining\n",
      "nextstate= 6\n",
      "reward= 0.2394097403080413\n",
      "env_action [0.0, 0.0]\n",
      "5  Evaluations Remaining\n",
      "nextstate= 2\n",
      "reward= 2.9509846098083132\n",
      "env_action [0.25, 0.97]\n",
      "4  Evaluations Remaining\n",
      "nextstate= 3\n",
      "reward= 50.51645538836125\n",
      "env_action [0.05, 0.0]\n",
      "3  Evaluations Remaining\n",
      "nextstate= 4\n",
      "reward= -0.004415408751204275\n",
      "env_action [0.92, 0.34]\n",
      "2  Evaluations Remaining\n",
      "nextstate= 5\n",
      "reward= 30.191372797429434\n",
      "env_action [0.09, 0.0]\n",
      "1  Evaluations Remaining\n",
      "nextstate= 6\n",
      "reward= -0.0924162371337216\n",
      "env_action [0.76, 0.73]\n",
      "0  Evaluations Remaining\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "You have exceeded the permitted number of evaluations",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-7b10dfb56432>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mEvaluateChallengeSubmission\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mChallengeSeqDecEnvironment\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mQ_Agent\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"Q_submission.csv\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python36\\site-packages\\netsapi\\challenge.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, environment, agent, filename, episode_number)\u001b[0m\n\u001b[0;32m    181\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mepisode_number\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mepisode_number\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    182\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m;\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 183\u001b[1;33m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscoringFunction\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    184\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcreate_submissions\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    185\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python36\\site-packages\\netsapi\\challenge.py\u001b[0m in \u001b[0;36mscoringFunction\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    194\u001b[0m             \u001b[0me\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menvironment\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    195\u001b[0m             \u001b[0ma\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0magent\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m;\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 196\u001b[1;33m             \u001b[0mfinalpolicy\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepisodicreward\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0ma\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    197\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpolicies\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfinalpolicy\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    198\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrewards\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepisodicreward\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-3-7b3d392c3c3d>\u001b[0m in \u001b[0;36mgenerate\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     65\u001b[0m         \u001b[0mbest_reward\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m-\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Inf'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 67\u001b[1;33m         \u001b[0mQ_trained\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     68\u001b[0m         \u001b[0mgreedy_eval\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mlambda\u001b[0m \u001b[0ms\u001b[0m \u001b[1;33m:\u001b[0m \u001b[0mmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mactionspace\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0ma\u001b[0m \u001b[1;33m:\u001b[0m \u001b[0mQ_trained\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-3-7b3d392c3c3d>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     47\u001b[0m                 \u001b[0menv_action\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mactions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;31m#convert to ITN/IRS\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     48\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'env_action'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0menv_action\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 49\u001b[1;33m                 \u001b[0mnextstate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mevaluateAction\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0menv_action\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     50\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"nextstate=\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mnextstate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     51\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"reward=\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mreward\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python36\\site-packages\\netsapi\\challenge.py\u001b[0m in \u001b[0;36mevaluateAction\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m    133\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimentsRemaining\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\" Evaluations Remaining\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    134\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimentsRemaining\u001b[0m \u001b[1;33m<=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 135\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'You have exceeded the permitted number of evaluations'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    136\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    137\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0many\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0many\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m<\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m>\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0maction\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: You have exceeded the permitted number of evaluations"
     ]
    }
   ],
   "source": [
    "EvaluateChallengeSubmission(ChallengeSeqDecEnvironment, Q_Agent, \"Q_submission.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BanditRPM(object):\n",
    "    def __init__(self,keys,init):\n",
    "        self.ActionValue = {}\n",
    "        for key in keys:\n",
    "            self.ActionValue[key] = init\n",
    "\n",
    "    def get_reward(self,action,text):\n",
    "        if any(x in text for x in action):\n",
    "            return 1\n",
    "        else:\n",
    "            return 0\n",
    "    \n",
    "    def choose_action(self):\n",
    "        \"\"\"\n",
    "        Use Thompson sampling to choose action. Sample from each posterior and choose the max of the samples.\n",
    "        \"\"\"\n",
    "        samples = {}\n",
    "        for key in self.ActionValue:\n",
    "            print(\"key=\",key)\n",
    "            print(\"key=\",self.ActionValue[key][0])\n",
    "            print(\"self.ActionValue[key][1]=\",self.ActionValue[key][1])\n",
    "            \n",
    "            samples[key] = np.random.beta(self.ActionValue[key][0], self.ActionValue[key][1])\n",
    "            print(\"samples[key]=\",samples[key])\n",
    "            max_value =  max(samples, key=samples.get)\n",
    "            print(\"max_value=\",max_value)\n",
    "            return max_value\n",
    "\n",
    "    def update(self,action,reward):\n",
    "        \"\"\"\n",
    "        Update parameters of posteriors, which are Beta distributions\n",
    "        \"\"\"\n",
    "        a, b = self.ActionValue[action]\n",
    "        self.ActionValue[action] = (a+reward, b + 1 - reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "bandit = BanditRPM([('hillary','clinton'),('donald','trump'),('bernie','sanders')],(1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{('hillary', 'clinton'): (1, 1),\n",
       " ('donald', 'trump'): (1, 1),\n",
       " ('bernie', 'sanders'): (1, 1)}"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bandit.ActionValue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "key= ('hillary', 'clinton')\n",
      "key= 1\n",
      "self.ActionValue[key][1]= 1\n",
      "samples[key]= 0.44087618596399186\n",
      "max_value= ('hillary', 'clinton')\n"
     ]
    }
   ],
   "source": [
    "action = bandit.choose_action()\n",
    "bandit.update(action,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
