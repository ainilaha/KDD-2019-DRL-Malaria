{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [KDD Cup|Humanities Track Tutorial Q-Learning](https://compete.hexagon-ml.com/tutorial/kdd-cuphumanities-track-tutorial/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KDD Cup|Humanities Track Tutorial Q-Learning\n",
    "This Tutorial builds on the previous tutorial to demonstrate a baseline implementation of a standard Reinforcement Learning (RL) Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### State\n",
    "\n",
    "$S \\in \\{1,2,3,4,5\\}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Action\n",
    "$A_S = [a_{ITN},a_{IRS}]$\n",
    "\n",
    "where  $a_{ITN} \\in [0,1]$ and $a_{IRS} \\in [0,1]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reward\n",
    "$R_{\\pi} \\in (- \\infty,\\infty)$\n",
    "\n",
    "![](image/rewards2.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "import random\n",
    "\n",
    "# !pip3 install git+https://github.com/slremy/netsapi --user --upgrade\n",
    "from netsapi.challenge import * "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0.0, 1.0],\n",
       " [0.1, 0.9],\n",
       " [0.2, 0.8],\n",
       " [0.3, 0.7],\n",
       " [0.4, 0.6],\n",
       " [0.5, 0.5],\n",
       " [0.6, 0.4],\n",
       " [0.7, 0.3],\n",
       " [0.8, 0.2],\n",
       " [0.9, 0.1],\n",
       " [1.0, 0.0]]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def actionSpace(resolution):\n",
    "    x = np.arange(0,1+resolution,resolution)\n",
    "    y = 1-x\n",
    "    x = x.reshape(len(x),1)\n",
    "    y = y.reshape(len(y),1)\n",
    "    xy = np.concatenate((x, y), axis=1)\n",
    "    return xy.round(2).tolist()\n",
    "actionSpace(0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning a Value Function Based on Ïµ-greedy action selection\n",
    "\n",
    "This common resource was used as a reference for the implementation presented here: https://kofzor.github.io/Learning_Value_Functions/. Please refer to the blog and this Tutorial in tandem. The code below uses the first example from the blog with the Challenge Environment (as opposed to Gym)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.0, 1.0], [0.1, 0.9], [0.2, 0.8], [0.3, 0.7], [0.4, 0.6], [0.5, 0.5], [0.6, 0.4], [0.7, 0.3], [0.8, 0.2], [0.9, 0.1], [1.0, 0.0]]\n",
      "env_action [0.0, 1.0]\n",
      "env_action <class 'list'>\n",
      "105  Evaluations Remaining\n",
      "reward= 93.54086115989467\n",
      "done= False\n",
      "env_action [0.0, 1.0]\n",
      "env_action <class 'list'>\n",
      "104  Evaluations Remaining\n",
      "reward= 0.252039612048792\n",
      "done= False\n",
      "env_action [0.0, 1.0]\n",
      "env_action <class 'list'>\n",
      "103  Evaluations Remaining\n",
      "reward= -0.09665503802477193\n",
      "done= False\n",
      "env_action [0.0, 1.0]\n",
      "env_action <class 'list'>\n",
      "102  Evaluations Remaining\n",
      "reward= -0.17210849514804005\n",
      "done= False\n",
      "env_action [0.0, 1.0]\n",
      "env_action <class 'list'>\n",
      "101  Evaluations Remaining\n",
      "reward= -0.17906876746628653\n",
      "done= True\n",
      "100  Evaluations Remaining\n",
      "{1: [0.0, 1.0], 2: [0.0, 1.0], 3: [0.0, 1.0], 4: [0.1, 0.9], 5: [0.1, 0.9]} 115.62394762460345\n"
     ]
    }
   ],
   "source": [
    "env = ChallengeSeqDecEnvironment(experimentCount = 105)\n",
    "\n",
    "Q = defaultdict(lambda : 0.) # Q-function\n",
    "n = defaultdict(lambda : 1.) # number of visits\n",
    "\n",
    "def actionSpace(resolution):\n",
    "    x = np.arange(0,1+resolution,resolution)\n",
    "    y = 1-x\n",
    "    x = x.reshape(len(x),1)\n",
    "    y = y.reshape(len(y),1)\n",
    "    xy = np.concatenate((x, y), axis=1)\n",
    "    return xy.round(2).tolist()\n",
    "\n",
    "\n",
    "#HyperParameters\n",
    "epsilon = 0.1\n",
    "gamma = 0.9\n",
    "action_resolution = 0.1\n",
    "episode_number = 1 #for submission this is fixed as 20\n",
    "\n",
    "\n",
    "#Set-up\n",
    "actions = actionSpace(action_resolution)\n",
    "print(actions)\n",
    "actionspace = range(len(actions)-1)\n",
    "greedy_action = lambda s : max(actionspace, key=lambda a : Q[(s,a)])\n",
    "max_q = lambda sp : max([Q[(sp,a)] for a in actionspace])\n",
    "\n",
    "#Training of Q Table\n",
    "for _ in range(episode_number):\n",
    "    env.reset()\n",
    "    nextstate = env.state\n",
    "    while True:\n",
    "        state = nextstate\n",
    "        # Epsilon-Greedy\n",
    "        if epsilon > random.random() :\n",
    "            action = random.choice(actionspace)\n",
    "            print('random_action',action)\n",
    "        else :\n",
    "            action = greedy_action(state)\n",
    "        env_action = actions[action]#convert to ITN/IRS\n",
    "        print('env_action', env_action)\n",
    "        print('env_action', type(env_action))\n",
    "        nextstate, reward, done, _ = env.evaluateAction(env_action)\n",
    "        print('reward=',reward)\n",
    "        print('done=',done)\n",
    "        \n",
    "\n",
    "        # Q-learning\n",
    "        if done :\n",
    "            Q[(state,action)] = Q[(state,action)] + \\\n",
    "                                1./n[(state,action)] * ( reward - Q[(state,action)] )\n",
    "            break\n",
    "        else :\n",
    "            Q[(state,action)] = Q[(state,action)] + \\\n",
    "                                1./n[(state,action)] * ( reward + \\\n",
    "                                                        gamma * max_q(nextstate) - Q[(state,action)] )\n",
    "\n",
    "#Greedy Policy Learnt from Q Table\n",
    "best_policy = {state: list(actions[greedy_action(state-1)]) for state in range(1,6)}\n",
    "best_reward = env.evaluatePolicy(best_policy)\n",
    "print(best_policy, best_reward)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a Valid Submission from Agent Code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Q_Agent():\n",
    "    \n",
    "    def __init__(self, environment):\n",
    "        \n",
    "        #Hyperparameters\n",
    "        self.env = environment\n",
    "        self.epsilon = 0.1\n",
    "        self.gamma = 0.9\n",
    "        self.action_resolution = 0.2\n",
    "        self.Q = defaultdict(lambda : 0.) # Q-function\n",
    "        self.n = defaultdict(lambda : 1.) # number of visits\n",
    "        self.actions = actionSpace(self.action_resolution)\n",
    "        self.actionspace = range(len(self.actions)-1)\n",
    "        \n",
    "    \n",
    "    def actionSpace(self,resolution):\n",
    "        x = np.arange(0,1+resolution,resolution)\n",
    "        y = 1-x\n",
    "        x = x.reshape(len(x),1)\n",
    "        y = y.reshape(len(y),1)\n",
    "        xy = np.concatenate((x, y), axis=1)\n",
    "        return xy.round(2).tolist()\n",
    "\n",
    "    def train(self):\n",
    "        \n",
    "        Q = self.Q\n",
    "        n = self.n\n",
    "        actions = self.actions\n",
    "        actionspace = self.actionspace\n",
    "\n",
    "        greedy_action = lambda s : max(actionspace, key=lambda a : Q[(s,a)])\n",
    "        max_q = lambda sp : max([Q[(sp,a)] for a in actionspace])\n",
    "\n",
    "        \n",
    "        for _ in range(20): #Do not change\n",
    "            \n",
    "            self.env.reset()\n",
    "            nextstate = self.env.state\n",
    "            \n",
    "            while True:\n",
    "                state = nextstate\n",
    "\n",
    "                # Epsilon-Greedy Action Selection\n",
    "                if epsilon > random.random() :\n",
    "                    action = random.choice(actionspace)\n",
    "                else :\n",
    "                    action = greedy_action(state)\n",
    "\n",
    "                env_action = actions[action]#convert to ITN/IRS\n",
    "                print('env_action', env_action)\n",
    "                nextstate, reward, done, _ = self.env.evaluateAction(env_action)\n",
    "                print(\"nextstate=\",nextstate)\n",
    "                print(\"reward=\",reward)\n",
    "\n",
    "                # Q-learning\n",
    "                if done :\n",
    "                    Q[(state,action)] = Q[(state,action)] + 1./n[(state,action)] * ( reward - Q[(state,action)] )\n",
    "                    break\n",
    "                else :\n",
    "                    Q[(state,action)] = Q[(state,action)] + 1./n[(state,action)] * ( reward + gamma * max_q(nextstate) - Q[(state,action)] )\n",
    "\n",
    "        return Q\n",
    "\n",
    "\n",
    "    def generate(self):\n",
    "        best_policy = None\n",
    "        best_reward = -float('Inf')\n",
    "        \n",
    "        Q_trained = self.train()\n",
    "        greedy_eval = lambda s : max(actionspace, key=lambda a : Q_trained[(s,a)])\n",
    "        \n",
    "        best_policy = {state: list(actions[greedy_eval(state-1)]) for state in range(1,6)}\n",
    "        best_reward = self.env.evaluatePolicy(best_policy)\n",
    "        \n",
    "        print(best_policy, best_reward)\n",
    "        I \n",
    "        return best_policy, best_reward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the EvaluateChallengeSubmission Method with your Agent Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env_action [0.0, 1.0]\n",
      "105  Evaluations Remaining\n",
      "nextstate= 2\n",
      "reward= 108.94641992310912\n",
      "env_action [0.2, 0.8]\n",
      "104  Evaluations Remaining\n",
      "nextstate= 3\n",
      "reward= 0.06355606938390368\n",
      "env_action [0.0, 1.0]\n",
      "103  Evaluations Remaining\n",
      "nextstate= 4\n",
      "reward= 38.91473211199865\n",
      "env_action [0.0, 1.0]\n",
      "102  Evaluations Remaining\n",
      "nextstate= 5\n",
      "reward= -0.016886570501970954\n",
      "env_action [0.2, 0.8]\n",
      "101  Evaluations Remaining\n",
      "nextstate= 6\n",
      "reward= -0.011054670432794733\n",
      "env_action [0.0, 1.0]\n",
      "100  Evaluations Remaining\n",
      "nextstate= 2\n",
      "reward= 105.33250965236144\n",
      "env_action [0.2, 0.8]\n",
      "99  Evaluations Remaining\n",
      "nextstate= 3\n",
      "reward= 0.09846357813729334\n",
      "env_action [0.0, 1.0]\n",
      "98  Evaluations Remaining\n",
      "nextstate= 4\n",
      "reward= 35.49995695541685\n",
      "env_action [0.2, 0.8]\n",
      "97  Evaluations Remaining\n",
      "nextstate= 5\n",
      "reward= 0.2279525809503964\n",
      "env_action [0.0, 1.0]\n",
      "96  Evaluations Remaining\n",
      "nextstate= 6\n",
      "reward= 36.49180972972042\n",
      "env_action [0.0, 1.0]\n",
      "95  Evaluations Remaining\n",
      "nextstate= 2\n",
      "reward= 110.65601922032198\n",
      "env_action [0.2, 0.8]\n",
      "94  Evaluations Remaining\n",
      "nextstate= 3\n",
      "reward= 0.20721673233602234\n",
      "env_action [0.0, 1.0]\n",
      "93  Evaluations Remaining\n",
      "nextstate= 4\n",
      "reward= 38.258682438580436\n",
      "env_action [0.6, 0.4]\n",
      "92  Evaluations Remaining\n",
      "nextstate= 5\n",
      "reward= 36.80559956544465\n",
      "env_action [0.0, 1.0]\n",
      "91  Evaluations Remaining\n",
      "nextstate= 6\n",
      "reward= 36.864907397666755\n",
      "env_action [0.0, 1.0]\n",
      "90  Evaluations Remaining\n",
      "nextstate= 2\n",
      "reward= 93.74276279742658\n",
      "env_action [0.2, 0.8]\n",
      "89  Evaluations Remaining\n",
      "nextstate= 3\n",
      "reward= 0.4073302474743654\n",
      "env_action [0.6, 0.4]\n",
      "88  Evaluations Remaining\n",
      "nextstate= 4\n",
      "reward= 13.889184998219509\n",
      "env_action [0.6, 0.4]\n",
      "87  Evaluations Remaining\n",
      "nextstate= 5\n",
      "reward= -27.81053467242034\n",
      "env_action [0.0, 1.0]\n",
      "86  Evaluations Remaining\n",
      "nextstate= 6\n",
      "reward= 35.562057312957094\n",
      "env_action [0.0, 1.0]\n",
      "85  Evaluations Remaining\n",
      "nextstate= 2\n",
      "reward= 106.14021797727251\n",
      "env_action [0.2, 0.8]\n",
      "84  Evaluations Remaining\n",
      "nextstate= 3\n",
      "reward= 0.2236766655967024\n",
      "env_action [0.6, 0.4]\n",
      "83  Evaluations Remaining\n",
      "nextstate= 4\n",
      "reward= 12.234536083301846\n",
      "env_action [0.6, 0.4]\n",
      "82  Evaluations Remaining\n",
      "nextstate= 5\n",
      "reward= -26.60226026587725\n",
      "env_action [0.0, 1.0]\n",
      "81  Evaluations Remaining\n",
      "nextstate= 6\n",
      "reward= 32.580210298230526\n",
      "env_action [0.0, 1.0]\n",
      "80  Evaluations Remaining\n",
      "nextstate= 2\n",
      "reward= 101.10971138371369\n",
      "env_action [0.2, 0.8]\n",
      "79  Evaluations Remaining\n",
      "nextstate= 3\n",
      "reward= 0.4373226018948757\n",
      "env_action [0.0, 1.0]\n",
      "78  Evaluations Remaining\n",
      "nextstate= 4\n",
      "reward= 37.11934770467042\n",
      "env_action [0.6, 0.4]\n",
      "77  Evaluations Remaining\n",
      "nextstate= 5\n",
      "reward= 34.46615860954522\n",
      "env_action [0.0, 1.0]\n",
      "76  Evaluations Remaining\n",
      "nextstate= 6\n",
      "reward= 38.91450159417248\n",
      "env_action [0.0, 1.0]\n",
      "75  Evaluations Remaining\n",
      "nextstate= 2\n",
      "reward= 94.39599994473981\n",
      "env_action [0.2, 0.8]\n",
      "74  Evaluations Remaining\n",
      "nextstate= 3\n",
      "reward= 0.13347754176723292\n",
      "env_action [0.0, 1.0]\n",
      "73  Evaluations Remaining\n",
      "nextstate= 4\n",
      "reward= 40.083622690099894\n",
      "env_action [0.6, 0.4]\n",
      "72  Evaluations Remaining\n",
      "nextstate= 5\n",
      "reward= 39.22894190214628\n",
      "env_action [0.0, 1.0]\n",
      "71  Evaluations Remaining\n",
      "nextstate= 6\n",
      "reward= 34.54718243387782\n",
      "env_action [0.0, 1.0]\n",
      "70  Evaluations Remaining\n",
      "nextstate= 2\n",
      "reward= 92.6113626594182\n",
      "env_action [0.2, 0.8]\n",
      "69  Evaluations Remaining\n",
      "nextstate= 3\n",
      "reward= 0.34692554431359923\n",
      "env_action [0.0, 1.0]\n",
      "68  Evaluations Remaining\n",
      "nextstate= 4\n",
      "reward= 40.71558987425256\n",
      "env_action [0.6, 0.4]\n",
      "67  Evaluations Remaining\n",
      "nextstate= 5\n",
      "reward= 37.42560365564037\n",
      "env_action [0.0, 1.0]\n",
      "66  Evaluations Remaining\n",
      "nextstate= 6\n",
      "reward= 32.53055897487923\n",
      "env_action [0.0, 1.0]\n",
      "65  Evaluations Remaining\n",
      "nextstate= 2\n",
      "reward= 93.33897284817319\n",
      "env_action [0.2, 0.8]\n",
      "64  Evaluations Remaining\n",
      "nextstate= 3\n",
      "reward= 0.35271089720754034\n",
      "env_action [0.0, 1.0]\n",
      "63  Evaluations Remaining\n",
      "nextstate= 4\n",
      "reward= 38.59308500411447\n",
      "env_action [0.6, 0.4]\n",
      "62  Evaluations Remaining\n",
      "nextstate= 5\n",
      "reward= 35.50074638885126\n",
      "env_action [0.0, 1.0]\n",
      "61  Evaluations Remaining\n",
      "nextstate= 6\n",
      "reward= 37.63614886015232\n",
      "env_action [0.0, 1.0]\n",
      "60  Evaluations Remaining\n",
      "nextstate= 2\n",
      "reward= 92.43831999969218\n",
      "env_action [0.2, 0.8]\n",
      "59  Evaluations Remaining\n",
      "nextstate= 3\n",
      "reward= 0.3211114938203865\n",
      "env_action [0.0, 1.0]\n",
      "58  Evaluations Remaining\n",
      "nextstate= 4\n",
      "reward= 42.165631332390866\n",
      "env_action [0.6, 0.4]\n",
      "57  Evaluations Remaining\n",
      "nextstate= 5\n",
      "reward= 36.43639514131953\n",
      "env_action [0.0, 1.0]\n",
      "56  Evaluations Remaining\n",
      "nextstate= 6\n",
      "reward= 31.762094679912583\n",
      "env_action [0.0, 1.0]\n",
      "55  Evaluations Remaining\n",
      "nextstate= 2\n",
      "reward= 97.56612946063247\n",
      "env_action [0.2, 0.8]\n",
      "54  Evaluations Remaining\n",
      "nextstate= 3\n",
      "reward= -0.01035676825360543\n",
      "env_action [0.0, 1.0]\n",
      "53  Evaluations Remaining\n",
      "nextstate= 4\n",
      "reward= 34.83195152859859\n",
      "env_action [0.6, 0.4]\n",
      "52  Evaluations Remaining\n",
      "nextstate= 5\n",
      "reward= 37.02396933871068\n",
      "env_action [0.0, 1.0]\n",
      "51  Evaluations Remaining\n",
      "nextstate= 6\n",
      "reward= 38.306573739104515\n",
      "env_action [0.0, 1.0]\n",
      "50  Evaluations Remaining\n",
      "nextstate= 2\n",
      "reward= 99.61427615200581\n",
      "env_action [0.2, 0.8]\n",
      "49  Evaluations Remaining\n",
      "nextstate= 3\n",
      "reward= 0.19606150425835045\n",
      "env_action [0.0, 1.0]\n",
      "48  Evaluations Remaining\n",
      "nextstate= 4\n",
      "reward= 36.025015170524476\n",
      "env_action [0.6, 0.4]\n",
      "47  Evaluations Remaining\n",
      "nextstate= 5\n",
      "reward= 39.25182715528099\n",
      "env_action [0.0, 1.0]\n",
      "46  Evaluations Remaining\n",
      "nextstate= 6\n",
      "reward= 33.68350775902352\n",
      "env_action [0.0, 1.0]\n",
      "45  Evaluations Remaining\n",
      "nextstate= 2\n",
      "reward= 104.32177294550144\n",
      "env_action [0.2, 0.8]\n",
      "44  Evaluations Remaining\n",
      "nextstate= 3\n",
      "reward= -0.04424111819228882\n",
      "env_action [0.0, 1.0]\n",
      "43  Evaluations Remaining\n",
      "nextstate= 4\n",
      "reward= 35.921220180991625\n",
      "env_action [0.6, 0.4]\n",
      "42  Evaluations Remaining\n",
      "nextstate= 5\n",
      "reward= 39.14221662750678\n",
      "env_action [0.0, 1.0]\n",
      "41  Evaluations Remaining\n",
      "nextstate= 6\n",
      "reward= 38.11221332631769\n",
      "env_action [0.0, 1.0]\n",
      "40  Evaluations Remaining\n",
      "nextstate= 2\n",
      "reward= 108.27778337887206\n",
      "env_action [0.2, 0.8]\n",
      "39  Evaluations Remaining\n",
      "nextstate= 3\n",
      "reward= -0.13027369929823562\n",
      "env_action [0.8, 0.2]\n",
      "38  Evaluations Remaining\n",
      "nextstate= 4\n",
      "reward= 37.408632810240505\n",
      "env_action [0.6, 0.4]\n",
      "37  Evaluations Remaining\n",
      "nextstate= 5\n",
      "reward= 2.263742624162188\n",
      "env_action [0.0, 1.0]\n",
      "36  Evaluations Remaining\n",
      "nextstate= 6\n",
      "reward= 36.66100109634424\n",
      "env_action [0.0, 1.0]\n",
      "35  Evaluations Remaining\n",
      "nextstate= 2\n",
      "reward= 92.94144371576415\n",
      "env_action [0.8, 0.2]\n",
      "34  Evaluations Remaining\n",
      "nextstate= 3\n",
      "reward= 63.26082879700905\n",
      "env_action [0.0, 1.0]\n",
      "33  Evaluations Remaining\n",
      "nextstate= 4\n",
      "reward= 113.7542500599489\n",
      "env_action [0.6, 0.4]\n",
      "32  Evaluations Remaining\n",
      "nextstate= 5\n",
      "reward= 34.40321821820579\n",
      "env_action [0.0, 1.0]\n",
      "31  Evaluations Remaining\n",
      "nextstate= 6\n",
      "reward= 34.6264178572978\n",
      "env_action [0.0, 1.0]\n",
      "30  Evaluations Remaining\n",
      "nextstate= 2\n",
      "reward= 104.77937102677626\n",
      "env_action [0.8, 0.2]\n",
      "29  Evaluations Remaining\n",
      "nextstate= 3\n",
      "reward= 61.50792358555751\n",
      "env_action [0.0, 1.0]\n",
      "28  Evaluations Remaining\n",
      "nextstate= 4\n",
      "reward= 109.17900982721659\n",
      "env_action [0.6, 0.4]\n",
      "27  Evaluations Remaining\n",
      "nextstate= 5\n",
      "reward= 34.55842927844561\n",
      "env_action [0.0, 1.0]\n",
      "26  Evaluations Remaining\n",
      "nextstate= 6\n",
      "reward= 38.42471677013733\n",
      "env_action [0.0, 1.0]\n",
      "25  Evaluations Remaining\n",
      "nextstate= 2\n",
      "reward= 107.504913005401\n",
      "env_action [0.8, 0.2]\n",
      "24  Evaluations Remaining\n",
      "nextstate= 3\n",
      "reward= 58.57242774874091\n",
      "env_action [0.0, 1.0]\n",
      "23  Evaluations Remaining\n",
      "nextstate= 4\n",
      "reward= 112.62099947310901\n",
      "env_action [0.6, 0.4]\n",
      "22  Evaluations Remaining\n",
      "nextstate= 5\n",
      "reward= 32.11760562295865\n",
      "env_action [0.0, 1.0]\n",
      "21  Evaluations Remaining\n",
      "nextstate= 6\n",
      "reward= 32.52752118305123\n",
      "env_action [0.0, 1.0]\n",
      "20  Evaluations Remaining\n",
      "nextstate= 2\n",
      "reward= 104.630628913986\n",
      "env_action [0.8, 0.2]\n",
      "19  Evaluations Remaining\n",
      "nextstate= 3\n",
      "reward= 54.62395371012131\n",
      "env_action [0.0, 1.0]\n",
      "18  Evaluations Remaining\n",
      "nextstate= 4\n",
      "reward= 103.17011032719451\n",
      "env_action [0.6, 0.4]\n",
      "17  Evaluations Remaining\n",
      "nextstate= 5\n",
      "reward= 38.54150807912556\n",
      "env_action [0.0, 1.0]\n",
      "16  Evaluations Remaining\n",
      "nextstate= 6\n",
      "reward= 36.727374158361464\n",
      "env_action [0.0, 1.0]\n",
      "15  Evaluations Remaining\n",
      "nextstate= 2\n",
      "reward= 100.67841648885249\n",
      "env_action [0.8, 0.2]\n",
      "14  Evaluations Remaining\n",
      "nextstate= 3\n",
      "reward= 65.84804757169405\n",
      "env_action [0.6, 0.4]\n",
      "13  Evaluations Remaining\n",
      "nextstate= 4\n",
      "reward= 2.6715874685096987\n",
      "env_action [0.6, 0.4]\n",
      "12  Evaluations Remaining\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nextstate= 5\n",
      "reward= -29.198947562882612\n",
      "env_action [0.8, 0.2]\n",
      "11  Evaluations Remaining\n",
      "nextstate= 6\n",
      "reward= -86.23689387031345\n",
      "env_action [0.0, 1.0]\n",
      "10  Evaluations Remaining\n",
      "nextstate= 2\n",
      "reward= 91.65472929701174\n",
      "env_action [0.8, 0.2]\n",
      "9  Evaluations Remaining\n",
      "nextstate= 3\n",
      "reward= 64.83425069532544\n",
      "env_action [0.0, 1.0]\n",
      "8  Evaluations Remaining\n",
      "nextstate= 4\n",
      "reward= 109.45544979599285\n",
      "env_action [0.6, 0.4]\n",
      "7  Evaluations Remaining\n",
      "nextstate= 5\n",
      "reward= 38.046884967583495\n",
      "env_action [0.0, 1.0]\n",
      "6  Evaluations Remaining\n",
      "nextstate= 6\n",
      "reward= 33.578959297425705\n",
      "5  Evaluations Remaining\n",
      "{1: [0.0, 1.0], 2: [0.0, 1.0], 3: [0.4, 0.6], 4: [0.0, 1.0], 5: [0.3, 0.7]} 134.37169372259612\n",
      "env_action [0.0, 1.0]\n",
      "105  Evaluations Remaining\n",
      "nextstate= 2\n",
      "reward= 108.58091464104668\n",
      "env_action [0.0, 1.0]\n",
      "104  Evaluations Remaining\n",
      "nextstate= 3\n",
      "reward= -0.042229894121489586\n",
      "env_action [0.0, 1.0]\n",
      "103  Evaluations Remaining\n",
      "nextstate= 4\n",
      "reward= 0.11935830009280002\n",
      "env_action [0.0, 1.0]\n",
      "102  Evaluations Remaining\n",
      "nextstate= 5\n",
      "reward= -0.07314743979967675\n",
      "env_action [0.0, 1.0]\n",
      "101  Evaluations Remaining\n",
      "nextstate= 6\n",
      "reward= -0.23633596555398695\n",
      "env_action [0.0, 1.0]\n",
      "100  Evaluations Remaining\n",
      "nextstate= 2\n",
      "reward= 98.80631260987961\n",
      "env_action [0.2, 0.8]\n",
      "99  Evaluations Remaining\n",
      "nextstate= 3\n",
      "reward= 0.2574019067161033\n",
      "env_action [0.0, 1.0]\n",
      "98  Evaluations Remaining\n",
      "nextstate= 4\n",
      "reward= 41.7614865978202\n",
      "env_action [0.2, 0.8]\n",
      "97  Evaluations Remaining\n",
      "nextstate= 5\n",
      "reward= 0.41562489482343823\n",
      "env_action [0.2, 0.8]\n",
      "96  Evaluations Remaining\n",
      "nextstate= 6\n",
      "reward= -6.9988254070149445\n",
      "env_action [0.0, 1.0]\n",
      "95  Evaluations Remaining\n",
      "nextstate= 2\n",
      "reward= 106.52532293620892\n",
      "env_action [0.2, 0.8]\n",
      "94  Evaluations Remaining\n",
      "nextstate= 3\n",
      "reward= -0.1331635446935091\n",
      "env_action [0.0, 1.0]\n",
      "93  Evaluations Remaining\n",
      "nextstate= 4\n",
      "reward= 40.62590431817959\n",
      "env_action [0.2, 0.8]\n",
      "92  Evaluations Remaining\n",
      "nextstate= 5\n",
      "reward= 0.02824089334187807\n",
      "env_action [0.4, 0.6]\n",
      "91  Evaluations Remaining\n",
      "nextstate= 6\n",
      "reward= -95.59428190879048\n",
      "env_action [0.0, 1.0]\n",
      "90  Evaluations Remaining\n",
      "nextstate= 2\n",
      "reward= 99.52157386340586\n",
      "env_action [0.2, 0.8]\n",
      "89  Evaluations Remaining\n",
      "nextstate= 3\n",
      "reward= 0.3410635198271672\n",
      "env_action [0.4, 0.6]\n",
      "88  Evaluations Remaining\n",
      "nextstate= 4\n",
      "reward= -82.8555575376274\n",
      "env_action [0.2, 0.8]\n",
      "87  Evaluations Remaining\n",
      "nextstate= 5\n",
      "reward= 2.2966425849018752\n",
      "env_action [0.6, 0.4]\n",
      "86  Evaluations Remaining\n",
      "nextstate= 6\n",
      "reward= 12.270629213187595\n",
      "env_action [0.0, 1.0]\n",
      "85  Evaluations Remaining\n",
      "nextstate= 2\n",
      "reward= 100.78930691644693\n",
      "env_action [0.2, 0.8]\n",
      "84  Evaluations Remaining\n",
      "nextstate= 3\n",
      "reward= 0.3534748379188688\n",
      "env_action [0.0, 1.0]\n",
      "83  Evaluations Remaining\n",
      "nextstate= 4\n",
      "reward= 39.13187296268078\n",
      "env_action [0.2, 0.8]\n",
      "82  Evaluations Remaining\n",
      "nextstate= 5\n",
      "reward= 0.20987435782736963\n",
      "env_action [0.6, 0.4]\n",
      "81  Evaluations Remaining\n",
      "nextstate= 6\n",
      "reward= 13.998987239255879\n",
      "env_action [0.0, 1.0]\n",
      "80  Evaluations Remaining\n",
      "nextstate= 2\n",
      "reward= 100.3635094662242\n",
      "env_action [0.2, 0.8]\n",
      "79  Evaluations Remaining\n",
      "nextstate= 3\n",
      "reward= -0.0075305959861835525\n",
      "env_action [0.0, 1.0]\n",
      "78  Evaluations Remaining\n",
      "nextstate= 4\n",
      "reward= 40.7307890780817\n",
      "env_action [0.2, 0.8]\n",
      "77  Evaluations Remaining\n",
      "nextstate= 5\n",
      "reward= 0.16525338955478475\n",
      "env_action [0.6, 0.4]\n",
      "76  Evaluations Remaining\n",
      "nextstate= 6\n",
      "reward= 13.923437318291768\n",
      "env_action [0.0, 1.0]\n",
      "75  Evaluations Remaining\n",
      "nextstate= 2\n",
      "reward= 104.75126424451186\n",
      "env_action [0.2, 0.8]\n",
      "74  Evaluations Remaining\n",
      "nextstate= 3\n",
      "reward= -0.003989893200242012\n",
      "env_action [0.0, 1.0]\n",
      "73  Evaluations Remaining\n",
      "nextstate= 4\n",
      "reward= 39.31976523182754\n",
      "env_action [0.2, 0.8]\n",
      "72  Evaluations Remaining\n",
      "nextstate= 5\n",
      "reward= 0.2009568964570363\n",
      "env_action [0.4, 0.6]\n",
      "71  Evaluations Remaining\n",
      "nextstate= 6\n",
      "reward= -98.98807123189923\n",
      "env_action [0.6, 0.4]\n",
      "70  Evaluations Remaining\n",
      "nextstate= 2\n",
      "reward= 5.917254540467483\n",
      "env_action [0.2, 0.8]\n",
      "69  Evaluations Remaining\n",
      "nextstate= 3\n",
      "reward= 16.20395862697199\n",
      "env_action [0.0, 1.0]\n",
      "68  Evaluations Remaining\n",
      "nextstate= 4\n",
      "reward= 41.41439377735744\n",
      "env_action [0.2, 0.8]\n",
      "67  Evaluations Remaining\n",
      "nextstate= 5\n",
      "reward= -0.03994464479641602\n",
      "env_action [0.6, 0.4]\n",
      "66  Evaluations Remaining\n",
      "nextstate= 6\n",
      "reward= 14.679021489976677\n",
      "env_action [0.0, 1.0]\n",
      "65  Evaluations Remaining\n",
      "nextstate= 2\n",
      "reward= 107.13206117873797\n",
      "env_action [0.2, 0.8]\n",
      "64  Evaluations Remaining\n",
      "nextstate= 3\n",
      "reward= 0.05032771848008011\n",
      "env_action [0.0, 1.0]\n",
      "63  Evaluations Remaining\n",
      "nextstate= 4\n",
      "reward= 37.410679918763314\n",
      "env_action [0.2, 0.8]\n",
      "62  Evaluations Remaining\n",
      "nextstate= 5\n",
      "reward= 0.26204438585356904\n",
      "env_action [0.6, 0.4]\n",
      "61  Evaluations Remaining\n",
      "nextstate= 6\n",
      "reward= 11.92067629484652\n",
      "env_action [0.0, 1.0]\n",
      "60  Evaluations Remaining\n",
      "nextstate= 2\n",
      "reward= 104.86312327002985\n",
      "env_action [0.2, 0.8]\n",
      "59  Evaluations Remaining\n",
      "nextstate= 3\n",
      "reward= 0.22515443825525283\n",
      "env_action [0.0, 1.0]\n",
      "58  Evaluations Remaining\n",
      "nextstate= 4\n",
      "reward= 39.20371574059703\n",
      "env_action [0.2, 0.8]\n",
      "57  Evaluations Remaining\n",
      "nextstate= 5\n",
      "reward= 0.25195924701098793\n",
      "env_action [0.6, 0.4]\n",
      "56  Evaluations Remaining\n",
      "nextstate= 6\n",
      "reward= 14.883401424933943\n",
      "env_action [0.0, 1.0]\n",
      "55  Evaluations Remaining\n",
      "nextstate= 2\n",
      "reward= 109.47482342602295\n",
      "env_action [0.2, 0.8]\n",
      "54  Evaluations Remaining\n",
      "nextstate= 3\n",
      "reward= 0.07214680817614294\n",
      "env_action [0.0, 1.0]\n",
      "53  Evaluations Remaining\n",
      "nextstate= 4\n",
      "reward= 41.356900682609485\n",
      "env_action [0.2, 0.8]\n",
      "52  Evaluations Remaining\n",
      "nextstate= 5\n",
      "reward= 0.09526469354807965\n",
      "env_action [0.6, 0.4]\n",
      "51  Evaluations Remaining\n",
      "nextstate= 6\n",
      "reward= 14.60934608312521\n",
      "env_action [0.0, 1.0]\n",
      "50  Evaluations Remaining\n",
      "nextstate= 2\n",
      "reward= 106.47683784474508\n",
      "env_action [0.2, 0.8]\n",
      "49  Evaluations Remaining\n",
      "nextstate= 3\n",
      "reward= 0.03469939427155255\n",
      "env_action [0.0, 1.0]\n",
      "48  Evaluations Remaining\n",
      "nextstate= 4\n",
      "reward= 38.47250986757782\n",
      "env_action [0.2, 0.8]\n",
      "47  Evaluations Remaining\n",
      "nextstate= 5\n",
      "reward= 0.022065013426396884\n",
      "env_action [0.6, 0.4]\n",
      "46  Evaluations Remaining\n",
      "nextstate= 6\n",
      "reward= 14.758982076757867\n",
      "env_action [0.4, 0.6]\n",
      "45  Evaluations Remaining\n",
      "nextstate= 2\n",
      "reward= 7.738152879199449\n",
      "env_action [0.2, 0.8]\n",
      "44  Evaluations Remaining\n",
      "nextstate= 3\n",
      "reward= 1.8948185313363735\n",
      "env_action [0.0, 1.0]\n",
      "43  Evaluations Remaining\n",
      "nextstate= 4\n",
      "reward= 39.70942881036941\n",
      "env_action [0.2, 0.8]\n",
      "42  Evaluations Remaining\n",
      "nextstate= 5\n",
      "reward= 0.2988955090037786\n",
      "env_action [0.6, 0.4]\n",
      "41  Evaluations Remaining\n",
      "nextstate= 6\n",
      "reward= 11.818839795985582\n",
      "env_action [0.0, 1.0]\n",
      "40  Evaluations Remaining\n",
      "nextstate= 2\n",
      "reward= 94.28943246063413\n",
      "env_action [0.2, 0.8]\n",
      "39  Evaluations Remaining\n",
      "nextstate= 3\n",
      "reward= 0.020118262279730903\n",
      "env_action [0.0, 1.0]\n",
      "38  Evaluations Remaining\n",
      "nextstate= 4\n",
      "reward= 42.11437628745971\n",
      "env_action [0.2, 0.8]\n",
      "37  Evaluations Remaining\n",
      "nextstate= 5\n",
      "reward= 0.38275247188959227\n",
      "env_action [0.6, 0.4]\n",
      "36  Evaluations Remaining\n",
      "nextstate= 6\n",
      "reward= 12.692629971091637\n",
      "env_action [0.4, 0.6]\n",
      "35  Evaluations Remaining\n",
      "nextstate= 2\n",
      "reward= 7.782131908237352\n",
      "env_action [0.2, 0.8]\n",
      "34  Evaluations Remaining\n",
      "nextstate= 3\n",
      "reward= 1.835576568127013\n",
      "env_action [0.0, 1.0]\n",
      "33  Evaluations Remaining\n",
      "nextstate= 4\n",
      "reward= 36.10757181626682\n",
      "env_action [0.2, 0.8]\n",
      "32  Evaluations Remaining\n",
      "nextstate= 5\n",
      "reward= 0.4249867985136637\n",
      "env_action [0.6, 0.4]\n",
      "31  Evaluations Remaining\n",
      "nextstate= 6\n",
      "reward= 12.066354694264577\n",
      "env_action [0.0, 1.0]\n",
      "30  Evaluations Remaining\n",
      "nextstate= 2\n",
      "reward= 110.38217901744697\n",
      "env_action [0.2, 0.8]\n",
      "29  Evaluations Remaining\n",
      "nextstate= 3\n",
      "reward= 0.12206064359838686\n",
      "env_action [0.0, 1.0]\n",
      "28  Evaluations Remaining\n",
      "nextstate= 4\n",
      "reward= 41.441843025812325\n",
      "env_action [0.2, 0.8]\n",
      "27  Evaluations Remaining\n",
      "nextstate= 5\n",
      "reward= 0.16713182044819463\n",
      "env_action [0.6, 0.4]\n",
      "26  Evaluations Remaining\n",
      "nextstate= 6\n",
      "reward= 12.304863984272613\n",
      "env_action [0.0, 1.0]\n",
      "25  Evaluations Remaining\n",
      "nextstate= 2\n",
      "reward= 106.31777355154708\n",
      "env_action [0.2, 0.8]\n",
      "24  Evaluations Remaining\n",
      "nextstate= 3\n",
      "reward= 0.11303964575931236\n",
      "env_action [0.0, 1.0]\n",
      "23  Evaluations Remaining\n",
      "nextstate= 4\n",
      "reward= 38.65648710926648\n",
      "env_action [0.8, 0.2]\n",
      "22  Evaluations Remaining\n",
      "nextstate= 5\n",
      "reward= 58.459629154513635\n",
      "env_action [0.6, 0.4]\n",
      "21  Evaluations Remaining\n",
      "nextstate= 6\n",
      "reward= 1.7587569452944636\n",
      "env_action [0.0, 1.0]\n",
      "20  Evaluations Remaining\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nextstate= 2\n",
      "reward= 95.5069022945814\n",
      "env_action [0.2, 0.8]\n",
      "19  Evaluations Remaining\n",
      "nextstate= 3\n",
      "reward= 0.2959618959254904\n",
      "env_action [0.0, 1.0]\n",
      "18  Evaluations Remaining\n",
      "nextstate= 4\n",
      "reward= 41.80830535963924\n",
      "env_action [0.8, 0.2]\n",
      "17  Evaluations Remaining\n",
      "nextstate= 5\n",
      "reward= 61.12728292093501\n",
      "env_action [0.6, 0.4]\n",
      "16  Evaluations Remaining\n",
      "nextstate= 6\n",
      "reward= 2.389299845958225\n",
      "env_action [0.0, 1.0]\n",
      "15  Evaluations Remaining\n",
      "nextstate= 2\n",
      "reward= 103.58755774366865\n",
      "env_action [0.2, 0.8]\n",
      "14  Evaluations Remaining\n",
      "nextstate= 3\n",
      "reward= 0.2014097249886988\n",
      "env_action [0.0, 1.0]\n",
      "13  Evaluations Remaining\n",
      "nextstate= 4\n",
      "reward= 35.962770406469915\n",
      "env_action [0.8, 0.2]\n",
      "12  Evaluations Remaining\n",
      "nextstate= 5\n",
      "reward= 64.40398116645541\n",
      "env_action [0.6, 0.4]\n",
      "11  Evaluations Remaining\n",
      "nextstate= 6\n",
      "reward= 1.754580359846487\n",
      "env_action [0.0, 1.0]\n",
      "10  Evaluations Remaining\n",
      "nextstate= 2\n",
      "reward= 107.37648837761975\n",
      "env_action [0.2, 0.8]\n",
      "9  Evaluations Remaining\n",
      "nextstate= 3\n",
      "reward= -0.00030508024736342065\n",
      "env_action [0.6, 0.4]\n",
      "8  Evaluations Remaining\n",
      "nextstate= 4\n",
      "reward= 14.26336424608343\n",
      "env_action [0.8, 0.2]\n",
      "7  Evaluations Remaining\n",
      "nextstate= 5\n",
      "reward= -94.28068510941222\n",
      "env_action [0.6, 0.4]\n",
      "6  Evaluations Remaining\n",
      "nextstate= 6\n",
      "reward= 2.533288724935804\n",
      "5  Evaluations Remaining\n",
      "{1: [0.0, 1.0], 2: [0.0, 1.0], 3: [0.1, 0.9], 4: [0.0, 1.0], 5: [0.1, 0.9]} 134.0074337126988\n",
      "env_action [0.0, 1.0]\n",
      "105  Evaluations Remaining\n",
      "nextstate= 2\n",
      "reward= 97.27539486625219\n",
      "env_action [0.0, 1.0]\n",
      "104  Evaluations Remaining\n",
      "nextstate= 3\n",
      "reward= -0.16139592552596138\n",
      "env_action [0.0, 1.0]\n",
      "103  Evaluations Remaining\n",
      "nextstate= 4\n",
      "reward= -0.07439685701536014\n",
      "env_action [0.0, 1.0]\n",
      "102  Evaluations Remaining\n",
      "nextstate= 5\n",
      "reward= -0.02686822801547306\n",
      "env_action [0.0, 1.0]\n",
      "101  Evaluations Remaining\n",
      "nextstate= 6\n",
      "reward= 0.057120496192007764\n",
      "env_action [0.0, 1.0]\n",
      "100  Evaluations Remaining\n",
      "nextstate= 2\n",
      "reward= 102.19688869591255\n",
      "env_action [0.2, 0.8]\n",
      "99  Evaluations Remaining\n",
      "nextstate= 3\n",
      "reward= 0.2035560377130916\n",
      "env_action [0.2, 0.8]\n",
      "98  Evaluations Remaining\n",
      "nextstate= 4\n",
      "reward= -7.184392436586499\n",
      "env_action [0.2, 0.8]\n",
      "97  Evaluations Remaining\n",
      "nextstate= 5\n",
      "reward= -7.050154386655926\n",
      "env_action [0.0, 1.0]\n",
      "96  Evaluations Remaining\n",
      "nextstate= 6\n",
      "reward= 35.16526192072797\n",
      "env_action [0.0, 1.0]\n",
      "95  Evaluations Remaining\n",
      "nextstate= 2\n",
      "reward= 111.75253744500208\n",
      "env_action [0.2, 0.8]\n",
      "94  Evaluations Remaining\n",
      "nextstate= 3\n",
      "reward= 0.4007085692419081\n",
      "env_action [0.4, 0.6]\n",
      "93  Evaluations Remaining\n",
      "nextstate= 4\n",
      "reward= -91.98393192697766\n",
      "env_action [0.4, 0.6]\n",
      "92  Evaluations Remaining\n",
      "nextstate= 5\n",
      "reward= -27.071405489269615\n",
      "env_action [0.0, 1.0]\n",
      "91  Evaluations Remaining\n",
      "nextstate= 6\n",
      "reward= 15.898425930208433\n",
      "env_action [0.0, 1.0]\n",
      "90  Evaluations Remaining\n",
      "nextstate= 2\n",
      "reward= 103.57619193839083\n",
      "env_action [0.2, 0.8]\n",
      "89  Evaluations Remaining\n",
      "nextstate= 3\n",
      "reward= 0.24830551402000633\n",
      "env_action [0.6, 0.4]\n",
      "88  Evaluations Remaining\n",
      "nextstate= 4\n",
      "reward= 14.664444268964271\n",
      "env_action [0.4, 0.6]\n",
      "87  Evaluations Remaining\n",
      "nextstate= 5\n",
      "reward= 2.2253843511080142\n",
      "env_action [0.0, 1.0]\n",
      "86  Evaluations Remaining\n",
      "nextstate= 6\n",
      "reward= 16.436252623290404\n",
      "env_action [0.0, 1.0]\n",
      "85  Evaluations Remaining\n",
      "nextstate= 2\n",
      "reward= 105.02719900603691\n",
      "env_action [0.2, 0.8]\n",
      "84  Evaluations Remaining\n",
      "nextstate= 3\n",
      "reward= -0.1335634701076418\n",
      "env_action [0.6, 0.4]\n",
      "83  Evaluations Remaining\n",
      "nextstate= 4\n",
      "reward= 13.051750626801951\n",
      "env_action [0.4, 0.6]\n",
      "82  Evaluations Remaining\n",
      "nextstate= 5\n",
      "reward= 2.8733382313911453\n",
      "env_action [0.0, 1.0]\n",
      "81  Evaluations Remaining\n",
      "nextstate= 6\n",
      "reward= 15.019593541821898\n",
      "env_action [0.4, 0.6]\n",
      "80  Evaluations Remaining\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-7b10dfb56432>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mEvaluateChallengeSubmission\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mChallengeSeqDecEnvironment\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mQ_Agent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Q_submission.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/netsapi/challenge.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, environment, agent, filename, episode_number)\u001b[0m\n\u001b[1;32m    181\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepisode_number\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mepisode_number\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 183\u001b[0;31m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscoringFunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    184\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_submissions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    185\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/netsapi/challenge.py\u001b[0m in \u001b[0;36mscoringFunction\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    194\u001b[0m             \u001b[0me\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menvironment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m             \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0magent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 196\u001b[0;31m             \u001b[0mfinalpolicy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepisodicreward\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    197\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpolicies\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfinalpolicy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrewards\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepisodicreward\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-7-f4ef2166b3b9>\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     67\u001b[0m         \u001b[0mbest_reward\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Inf'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m         \u001b[0mQ_trained\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m         \u001b[0mgreedy_eval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0ms\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactionspace\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0ma\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mQ_trained\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-7-f4ef2166b3b9>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     49\u001b[0m                 \u001b[0menv_action\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mactions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;31m#convert to ITN/IRS\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'env_action'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menv_action\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m                 \u001b[0mnextstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluateAction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv_action\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"nextstate=\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnextstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"reward=\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mreward\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/netsapi/challenge.py\u001b[0m in \u001b[0;36mevaluateAction\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    145\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m~\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpolicyDimension\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 147\u001b[0;31m             \u001b[0mreward\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_simplePostAction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    148\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/netsapi/challenge.py\u001b[0m in \u001b[0;36m_simplePostAction\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    108\u001b[0m             \u001b[0mextended_action\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'state'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m             \u001b[0;31m#print(extended_action)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 110\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrequests\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpost\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrewardUrl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdumps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mextended_action\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheaders\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'Content-Type'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'application/json'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Accept'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'application/json'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    111\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m             \u001b[0mreward\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'data'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/kdd2019/lib/python3.6/site-packages/requests/api.py\u001b[0m in \u001b[0;36mpost\u001b[0;34m(url, data, json, **kwargs)\u001b[0m\n\u001b[1;32m    114\u001b[0m     \"\"\"\n\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'post'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/kdd2019/lib/python3.6/site-packages/requests/api.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(method, url, **kwargs)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0;31m# cases, and look like a memory leak in others.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0msessions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/kdd2019/lib/python3.6/site-packages/requests/sessions.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    531\u001b[0m         }\n\u001b[1;32m    532\u001b[0m         \u001b[0msend_kwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msettings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 533\u001b[0;31m         \u001b[0mresp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0msend_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    534\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    535\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/kdd2019/lib/python3.6/site-packages/requests/sessions.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    644\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    645\u001b[0m         \u001b[0;31m# Send the request\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 646\u001b[0;31m         \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0madapter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    647\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    648\u001b[0m         \u001b[0;31m# Total elapsed time of the request (approximately)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/kdd2019/lib/python3.6/site-packages/requests/adapters.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    447\u001b[0m                     \u001b[0mdecode_content\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    448\u001b[0m                     \u001b[0mretries\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_retries\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 449\u001b[0;31m                     \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    450\u001b[0m                 )\n\u001b[1;32m    451\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/kdd2019/lib/python3.6/site-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36murlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[1;32m    601\u001b[0m                                                   \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout_obj\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    602\u001b[0m                                                   \u001b[0mbody\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 603\u001b[0;31m                                                   chunked=chunked)\n\u001b[0m\u001b[1;32m    604\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    605\u001b[0m             \u001b[0;31m# If we're going to release the connection in ``finally:``, then\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/kdd2019/lib/python3.6/site-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36m_make_request\u001b[0;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[1;32m    342\u001b[0m         \u001b[0;31m# Trigger any extra validation we need to do.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    343\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 344\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_conn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    345\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mSocketTimeout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBaseSSLError\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    346\u001b[0m             \u001b[0;31m# Py2 raises this as a BaseSSLError, Py3 raises it as socket timeout.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/kdd2019/lib/python3.6/site-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36m_validate_conn\u001b[0;34m(self, conn)\u001b[0m\n\u001b[1;32m    841\u001b[0m         \u001b[0;31m# Force connect early to allow us to validate the connection.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    842\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'sock'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# AppEngine might not have  `.sock`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 843\u001b[0;31m             \u001b[0mconn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconnect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    844\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    845\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mconn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_verified\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/kdd2019/lib/python3.6/site-packages/urllib3/connection.py\u001b[0m in \u001b[0;36mconnect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    314\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mconnect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    315\u001b[0m         \u001b[0;31m# Add certificate verification\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 316\u001b[0;31m         \u001b[0mconn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_new_conn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    317\u001b[0m         \u001b[0mhostname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhost\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    318\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/kdd2019/lib/python3.6/site-packages/urllib3/connection.py\u001b[0m in \u001b[0;36m_new_conn\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    158\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m             conn = connection.create_connection(\n\u001b[0;32m--> 160\u001b[0;31m                 (self._dns_host, self.port), self.timeout, **extra_kw)\n\u001b[0m\u001b[1;32m    161\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mSocketTimeout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/kdd2019/lib/python3.6/site-packages/urllib3/util/connection.py\u001b[0m in \u001b[0;36mcreate_connection\u001b[0;34m(address, timeout, source_address, socket_options)\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0msource_address\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m                 \u001b[0msock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msource_address\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m             \u001b[0msock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconnect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msa\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0msock\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "EvaluateChallengeSubmission(ChallengeSeqDecEnvironment, Q_Agent, \"Q_submission.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
