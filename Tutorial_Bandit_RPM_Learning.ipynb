{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [KDD Cup|Humanities Track Tutorial Q-Learning](https://compete.hexagon-ml.com/tutorial/kdd-cuphumanities-track-tutorial/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KDD Cup|Humanities Track Tutorial Q-Learning\n",
    "This Tutorial builds on the previous tutorial to demonstrate a baseline implementation of a standard Reinforcement Learning (RL) Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### State\n",
    "\n",
    "$S \\in \\{1,2,3,4,5\\}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Action\n",
    "$A_S = [a_{ITN},a_{IRS}]$\n",
    "\n",
    "where  $a_{ITN} \\in [0,1]$ and $a_{IRS} \\in [0,1]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reward\n",
    "$R_{\\pi} \\in (- \\infty,\\infty)$\n",
    "\n",
    "![](image/rewards2.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "import random\n",
    "# !pip3 install git+https://github.com/slremy/netsapi --user --upgrade\n",
    "from netsapi.challenge import * "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a Valid Submission from Agent Code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BanditRPM(object):\n",
    "    def __init__(self,env):\n",
    "        self.env = env\n",
    "        self.action_resolution = 0.1\n",
    "        self.actions = self.actionSpace()    \n",
    "        \n",
    "        self.ActionValue = {}\n",
    "        self.init = (2,5)\n",
    "        for key in self.actions:\n",
    "            self.ActionValue[key] = self.init\n",
    "        \n",
    "#     def actionSpace(self):\n",
    "#         x = np.arange(0,1+self.action_resolution,self.action_resolution)\n",
    "#         y = 1-x\n",
    "#         x = x.reshape(len(x),1)\n",
    "#         y = y.reshape(len(y),1)\n",
    "#         xy = np.concatenate((x, y), axis=1)\n",
    "#         xy = xy.round(2)\n",
    "#         xy = [tuple(row) for row in xy]\n",
    "        \n",
    "#         return xy\n",
    "        \n",
    "    def actionSpace(self):\n",
    "        x,y = np.meshgrid(np.arange(0,1+self.action_resolution,self.action_resolution),\n",
    "                          np.arange(0,1+self.action_resolution,self.action_resolution))\n",
    "        xy = np.concatenate((x.reshape(-1,1), y.reshape(-1,1)), axis=1)\n",
    "        xy = xy.round(2)\n",
    "        xy = [tuple(row) for row in xy]\n",
    "        return xy\n",
    "    \n",
    "    def choose_action(self):\n",
    "        \"\"\"\n",
    "        Use Thompson sampling to choose action. Sample from each posterior and choose the max of the samples.\n",
    "        \"\"\"\n",
    "        samples = {}\n",
    "        for key in self.ActionValue:\n",
    "            samples[key] = np.random.beta(self.ActionValue[key][0], self.ActionValue[key][1])\n",
    "        max_value =  max(samples, key=samples.get)\n",
    "        return max_value    \n",
    "\n",
    "    def update(self,action,reward):\n",
    "        \"\"\"\n",
    "        Update parameters of posteriors, which are Beta distributions\n",
    "        \"\"\"\n",
    "        a, b = self.ActionValue[action]\n",
    "        a = a+reward\n",
    "        b = b + 1 - reward\n",
    "        a = 0.001 if a <= 0 else a\n",
    "        b = 0.001 if b <= 0 else b\n",
    "        \n",
    "        self.ActionValue[action] = (a, b)\n",
    "        \n",
    "    def train(self):\n",
    "        for _ in range(20): #Do not change\n",
    "            self.env.reset()\n",
    "            while True:\n",
    "                action =  self.choose_action()\n",
    "                nextstate, reward, done, _ = self.env.evaluateAction(list(action))\n",
    "                self.update(action,reward)\n",
    "                if done:\n",
    "                    break\n",
    "\n",
    "\n",
    "    def generate(self):\n",
    "        best_policy = None\n",
    "        best_reward = -float('Inf')\n",
    "        self.train()\n",
    "        best_policy = {state: list(self.choose_action()) for state in range(1,6)}\n",
    "        best_reward = self.env.evaluatePolicy(best_policy)\n",
    "        \n",
    "        print(best_policy, best_reward)\n",
    "        \n",
    "        return best_policy, best_reward                    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the EvaluateChallengeSubmission Method with your Agent Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EvaluateChallengeSubmission(ChallengeSeqDecEnvironment, BanditRPM, \"BanditRPM_submission.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BanditRPM(object):\n",
    "    def __init__(self,keys,init):\n",
    "        self.ActionValue = {}\n",
    "        for key in keys:\n",
    "            self.ActionValue[key] = init\n",
    "    \n",
    "    def get_reward(self,action,text):\n",
    "        print(\"action=\",action)\n",
    "        print(\"text=\",text)\n",
    "        if any(x in text for x in action):\n",
    "            return 1\n",
    "        else:\n",
    "            return 0\n",
    "    \n",
    "    def choose_action(self):\n",
    "        \"\"\"\n",
    "        Use Thompson sampling to choose action. Sample from each posterior and choose the max of the samples.\n",
    "        \"\"\"\n",
    "        samples = {}\n",
    "        for key in self.ActionValue:\n",
    "            print(\"key=\",key)\n",
    "#             print(\"key=\",self.ActionValue[key][0])\n",
    "#             print(\"self.ActionValue[key][1]=\",self.ActionValue[key][1])\n",
    "            \n",
    "            samples[key] = np.random.beta(self.ActionValue[key][0], self.ActionValue[key][1])\n",
    "#             print(\"samples[key]=\",samples[key])\n",
    "        max_value =  max(samples, key=samples.get)\n",
    "        print(\"max_value=\",max_value)\n",
    "        return max_value\n",
    "\n",
    "    def update(self,action,reward):\n",
    "        \"\"\"\n",
    "        Update parameters of posteriors, which are Beta distributions\n",
    "        \"\"\"\n",
    "        print(\"action=\",action)\n",
    "        print(\"reward=\",reward)\n",
    "        a, b = self.ActionValue[action]\n",
    "        a = a+reward\n",
    "        b = b + 1 - reward\n",
    "        a = 0.0001 if a <= 0 else a\n",
    "        b = 0.0001 if b <= 0 else b\n",
    "        print(\"a=\",a)\n",
    "        print(\"b=\",b)\n",
    "        self.ActionValue[action] = (a, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "bandit = BanditRPM([('hillary','clinton'),('donald','trump'),('bernie','sanders')],(1,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{('hillary', 'clinton'): (1, 5),\n",
       " ('donald', 'trump'): (1, 5),\n",
       " ('bernie', 'sanders'): (1, 5)}"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bandit.ActionValue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_value= ('hillary', 'clinton')\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "get_reward() missing 1 required positional argument: 'text'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-112-18c1d55566de>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0maction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbandit\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchoose_action\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mreward\u001b[0m\u001b[1;33m=\u001b[0m \u001b[0mbandit\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_reward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mbandit\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mreward\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: get_reward() missing 1 required positional argument: 'text'"
     ]
    }
   ],
   "source": [
    "action = bandit.choose_action()\n",
    "reward= bandit.get_reward(action)\n",
    "bandit.update(action,reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
