{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# [KDD Cup|Humanities Track Tutorial](https://compete.hexagon-ml.com/tutorial/kdd-cuphumanities-track-tutorial/)\n",
    "This notebook by Oetbent\n",
    "\n",
    "This Tutorial exposes participants on approaches to learn a sequence of intervention based decisions (Actions) from a malaria modelling environment. We'll introduce notions of State, Action and Reward, in order to descibe some approaches to this challenge."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. State\n",
    "\n",
    "Observations for the challenge models occur over a 5 year timeframe and each year of this timeframe may be considered as the State of the system. With the possiblity to take one Action for each State. While it should also be noted this temporal State transition is fixed and as such not dependant on the Action taken.\n",
    "\n",
    "$S \\in \\{1,2,3,4,5\\}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.Action\n",
    "Consider Actions as a combination of only two possibile interventions i.e. Insecticide spraying(IRS) and distributing bed nets(ITN) based on our model description. $a_{ITN} \\in [0,1]$ and $a_{IRS} \\in [0,1]$. Action values between O and 1 describe a coverage range of the intervention for a simulated human population.\n",
    "\n",
    "$A_S = [a_{ITN},a_{IRS}]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Reward\n",
    "A reward function determines a Stochastic Reward for a Policy over the entire episode, this function acts to determine the Health outcomes per unit cost for the interventions implementated in the policy. In order to have a notion of goodness maximising the Reward we negate this value.\n",
    "\n",
    "$R_{\\pi} \\in (- \\infty,\\infty)$\n",
    "\n",
    "![](image/reward.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Policy\n",
    "Therefore a Policy($\\pi$) for this challenge consists of a temporal sequence of Actions, defined in the code as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'1': [0.55, 0.7], '2': [0, 0], '3': [0, 0], '4': [0, 0], '5': [0, 0]}]\n"
     ]
    }
   ],
   "source": [
    "policies = []\n",
    "policy = {}\n",
    "\n",
    "policy['1']=[.55,.7]\n",
    "policy['2']=[0,0]\n",
    "policy['3']=[0,0]\n",
    "policy['4']=[0,0]\n",
    "policy['5']=[0,0]\n",
    "\n",
    "policies.append(policy)\n",
    "print(policies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from sys import exit, exc_info, argv\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from netsapi.challenge import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating Policies as a Stochastic Multi-armed Bandit\n",
    "#### Evaluating Policies as a Sequence of Actions :\n",
    "In this section, we run several experiments with the same Policy and visualize the Rewards to indicate the stochasticity with MatPlotLib visualisations\n",
    "#### Evaluating a single known Policy\n",
    "Let's start with a current intervention campaign 55% ITN and 70% IRS coverage and obtain our reward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.55, 0.7]\n",
      "100  Evaluations Remaining\n",
      "12.445943665240458\n"
     ]
    }
   ],
   "source": [
    "print(policies[0]['1']) #Action in Year 1\n",
    "\n",
    "env = ChallengeEnvironment() #Initialise Challenge Environment\n",
    "reward = env.evaluateReward(np.asarray(policies[0]['1'])) #This has been negated and any reward should be maximised\n",
    "\n",
    "print(reward)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Update to Sequential Decision Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(policies[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "105  Evaluations Remaining\n",
      "11.671068641508104\n"
     ]
    }
   ],
   "source": [
    "envSeqDec = ChallengeSeqDecEnvironment() #Initialise a New Challenge Environment to post entire policy\n",
    "reward = envSeqDec.evaluatePolicy(policies[0]) #Action in Year 1 only\n",
    "\n",
    "print(reward)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Observing Stochastic Rewards for a single policy\n",
    "\n",
    "Let's repeat evaluations for the policy above and visualise the rewards as a boxplot (MatPlotLib may be required but you are free to visualise the data how you see fit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100  Evaluations Remaining\n",
      "95  Evaluations Remaining\n",
      "90  Evaluations Remaining\n",
      "85  Evaluations Remaining\n",
      "80  Evaluations Remaining\n",
      "75  Evaluations Remaining\n",
      "70  Evaluations Remaining\n",
      "65  Evaluations Remaining\n",
      "60  Evaluations Remaining\n",
      "55  Evaluations Remaining\n",
      "[11.67106864 12.42047633 12.33634885 13.66359014 11.47268793 13.10637832\n",
      " 13.69842717 11.97039387 13.17232769 11.63599918 12.95242353]\n"
     ]
    }
   ],
   "source": [
    "rewards = [reward]\n",
    "for i in range(10):   \n",
    "    reward = envSeqDec.evaluatePolicy(policies[0])\n",
    "    rewards = np.append(rewards, reward)\n",
    "    \n",
    "print(rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.boxplot(rewards)\n",
    "plt.xlabel('Action [ITN,IRS]')\n",
    "plt.ylabel('Rewards')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluating Policies as a Sequence of Actions\n",
    "\n",
    "High performing Policies may also be found through framing this as a sequential decision making problem and so this may be extended to the reinforcement learning paradigm as described above. Rewards can be returned for each state, action pair. Observations are the simplest case and only the year in which the decision is being made, rewards may be observed based on an action taken in each state in the sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50  Evaluations Remaining\n",
      "reward 106.9431245798812\n",
      "49  Evaluations Remaining\n",
      "reward -0.06803372864798751\n",
      "48  Evaluations Remaining\n",
      "reward -0.14328386438833762\n",
      "47  Evaluations Remaining\n",
      "reward -0.254236268164751\n",
      "46  Evaluations Remaining\n",
      "reward -1.321089717681929\n",
      "policy {'1': [0.0, 1.0], '2': [0.12861708692009788, 0.9916943304023614], '3': [0.06798125744018745, 0.9976865984049555], '4': [0.14279409144721586, 0.98975241724775], '5': [0.25150629987585954, 0.9678556613063511]}\n",
      "episodic_reward 105.1564810009982\n"
     ]
    }
   ],
   "source": [
    "episode_count = 1 \n",
    "\n",
    "reward = 0\n",
    "for i in range(episode_count):\n",
    "    envSeqDec.reset()\n",
    "    episodic_reward = 0\n",
    "\n",
    "    while True:\n",
    "        \n",
    "        #Agent Training Code here\n",
    "        action = [abs(np.sin(reward)),abs(np.cos(reward))] \n",
    "        envSeqDec.policy[str(envSeqDec.state)] = action \n",
    "        \n",
    "        ob, reward, done, _ = envSeqDec.evaluateAction(action)\n",
    "        print('reward',reward)\n",
    "        episodic_reward += reward\n",
    "        \n",
    "        if done:\n",
    "            break\n",
    "    \n",
    "    policies.append(envSeqDec.policy)\n",
    "    print('policy', envSeqDec.policy)\n",
    "    print('episodic_reward', episodic_reward)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating a Submission file\n",
    "#### Description of Submission Process\n",
    "Please do not alter the methods outside of the generate() method. This code is open for your own Agent implementation. The submission file to be scored and generated is based on 10 runs of 20 episodes in which a policy of 5 actions is run, this meets the 1000 evaluations constraint of the initialised environment. When you package your code for evaluation it will be required that you maintain this style."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sys import exit, exc_info, argv\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from netsapi.challenge import *\n",
    "\n",
    "class CustomAgent:\n",
    "    def __init__(self, environment):\n",
    "        self.environment = environment\n",
    "\n",
    "    def generate(self):\n",
    "        best_policy = None\n",
    "        best_reward = -float('Inf')\n",
    "        candidates = []\n",
    "        try:\n",
    "            # Agents should make use of 20 episodes in each training run, if making sequential decisions\n",
    "            for i in range(20):\n",
    "                self.environment.reset()\n",
    "                policy = {}\n",
    "                for j in range(5): #episode length\n",
    "                    policy[str(j+1)]=[0,1]\n",
    "                candidates.append(policy)\n",
    "                \n",
    "            rewards = self.environment.evaluatePolicy(candidates)\n",
    "            best_policy = candidates[np.argmax(rewards)]\n",
    "            best_reward = rewards[np.argmax(rewards)]\n",
    "        \n",
    "        except (KeyboardInterrupt, SystemExit):\n",
    "            print(exc_info())\n",
    "            \n",
    "        return best_policy, best_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "105  Evaluations Remaining\n",
      "105  Evaluations Remaining\n",
      "105  Evaluations Remaining\n",
      "105  Evaluations Remaining\n",
      "105  Evaluations Remaining\n",
      "105  Evaluations Remaining\n",
      "105  Evaluations Remaining\n",
      "105  Evaluations Remaining\n",
      "105  Evaluations Remaining\n",
      "105  Evaluations Remaining\n",
      "110.84030965146971\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<netsapi.challenge.EvaluateChallengeSubmission at 0x26889a9e710>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "EvaluateChallengeSubmission(ChallengeSeqDecEnvironment, CustomAgent, \"tutorial.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
